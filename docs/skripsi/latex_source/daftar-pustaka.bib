%1
@article{seymour2011history,
  title = {History of search engines},
  author = {Seymour, Tom and Frantsvog, Dean and Kumar, Satheesh},
  journal = {International Journal of Management \& Information Systems (IJMIS)},
  volume = {15},
  number = {4},
  pages = {47--58},
  year = {2011},
}

%2
@article{brin1998can,
  title = {What can you do with a web in your pocket?},
  author = {Brin, Sergey and Motwani, Rajeev and Page, Lawrence and Winograd,
            Terry},
  journal = {IEEE Data Eng. Bull.},
  volume = {21},
  number = {2},
  pages = {37--47},
  year = {1998},
  publisher = {Citeseer},
}

%3
@article{zhang2008web,
  title = {Web mining: a survey of current research, techniques, and software},
  author = {Zhang, Qingyu and Segall, Richard S},
  journal = {International Journal of Information Technology \& Decision Making},
  volume = {7},
  number = {04},
  pages = {683--720},
  year = {2008},
  publisher = {World Scientific},
}

%4
@article{brin1998anatomy,
  title = {The anatomy of a large-scale hypertextual web search engine},
  author = {Brin, Sergey and Page, Lawrence},
  year = {1998},
}

@article{multithreadedtextsearch,
  title = {On Multi-Thread Crawler Optimization for Scalable Text Searching},
  author = {Sun, Guang and Xiang, Huanxing and Li, Shuanghu},
  year = {2019},
}

@article{Pramudita_2020,
  year = {2020},
  month = {jul},
  publisher = {IOP Publishing},
  volume = {1569},
  number = {2},
  pages = {022077},
  author = {Y D Pramudita and D R Anamisa and S S Putro and M A Rahmawanto},
  title = {Extraction System Web Content Sports New Based On Web Crawler Multi
           Thread},
  journal = {Journal of Physics: Conference Series},
  abstract = {Web crawlers are programs that are used by search engines to
              collect necessary information from the internet automatically
              according to the rules set by the user. With so much information
              about sports news on the internet, it takes web crawlers with
              incredible speed in the process of crawling. There are several
              previous studies that discussed the process of extracting
              information in a web document that needs to be considered both in
              terms of both aspects, including in terms of the structure of the
              web page and the length of time needed. Therefore, in this research
              the web crawler application was developed by applying a
              multi-thread approach. This multi-thread approach to research is
              used to produce web crawlers that are faster in the process of
              crawling sports news by involving news sources more than one
              address at a time. In addition to the multi-thread approach,
              adjusting the structure of the website pages is also done to ensure
              the information to be extracted by web crawling. From the results
              of the multi-thread implementation test on the crawling process,
              this study has been able to increase speed compared to the
              single-thread method of 122.95 seconds. But the results of web
              update detection, have resulted in a speed that decreased by 6.27
              seconds in the crawling process with unequal data and the speed on
              the crawling process has also decreased by 24.76 seconds on server
              1 and by 23.92 seconds on server 2.},
}

@article{RustPerformance,
  author = {Lin, Yi and Blackburn, Stephen M. and Hosking, Antony L. and Norrish
            , Michael},
  title = {Rust as a language for high performance GC implementation},
  year = {2016},
  issue_date = {November 2016},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {51},
  number = {11},
  issn = {0362-1340},
  url = {https://doi.org/10.1145/3241624.2926707},
  doi = {10.1145/3241624.2926707},
  abstract = {High performance garbage collectors build upon
              performance-critical low-level code, typically exhibit multiple
              levels of concurrency, and are prone to subtle bugs. Implementing,
              debugging and maintaining such collectors can therefore be
              extremely challenging. The choice of implementation language is a
              crucial consideration when building a collector. Typically, the
              drive for performance and the need for efficient support of
              low-level memory operations leads to the use of low-level languages
              like C or C++, which offer little by way of safety and software
              engineering benefits. This risks undermining the robustness and
              flexibility of the collector design. Rust's ownership model,
              lifetime specification, and reference borrowing deliver safety
              guarantees through a powerful static checker with little runtime
              overhead. These features make Rust a compelling candidate for a
              collector implementation language, but they come with restrictions
              that threaten expressiveness and efficiency. We describe our
              experience implementing an Immix garbage collector in Rust and C.
              We discuss the benefits of Rust, the obstacles encountered, and how
              we overcame them. We show that our Immix implementation has almost
              identical performance on micro benchmarks, compared to its
              implementation in C, and outperforms the popular BDW collector on
              the gcbench micro benchmark. We find that Rust's safety features do
              not create significant barriers to implementing a high performance
              collector. Though memory managers are usually considered low-level,
              our high performance implementation relies on very little unsafe
              code, with the vast majority of the implementation benefiting from
              Rust's safety. We see our experience as a compelling
              proof-of-concept of Rust as an implementation language for high
              performance garbage collection.},
  journal = {SIGPLAN Not.},
  month = {jun},
  pages = {89–98},
  numpages = {10},
  keywords = {memory management, garbage collection, Rust},
}

@inproceedings{10.1145/2926697.2926707,
  author = {Lin, Yi and Blackburn, Stephen M. and Hosking, Antony L. and Norrish
            , Michael},
  title = {Rust as a language for high performance GC implementation},
  year = {2016},
  isbn = {9781450343176},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2926697.2926707},
  doi = {10.1145/2926697.2926707},
  abstract = {High performance garbage collectors build upon
              performance-critical low-level code, typically exhibit multiple
              levels of concurrency, and are prone to subtle bugs. Implementing,
              debugging and maintaining such collectors can therefore be
              extremely challenging. The choice of implementation language is a
              crucial consideration when building a collector. Typically, the
              drive for performance and the need for efficient support of
              low-level memory operations leads to the use of low-level languages
              like C or C++, which offer little by way of safety and software
              engineering benefits. This risks undermining the robustness and
              flexibility of the collector design. Rust's ownership model,
              lifetime specification, and reference borrowing deliver safety
              guarantees through a powerful static checker with little runtime
              overhead. These features make Rust a compelling candidate for a
              collector implementation language, but they come with restrictions
              that threaten expressiveness and efficiency. We describe our
              experience implementing an Immix garbage collector in Rust and C.
              We discuss the benefits of Rust, the obstacles encountered, and how
              we overcame them. We show that our Immix implementation has almost
              identical performance on micro benchmarks, compared to its
              implementation in C, and outperforms the popular BDW collector on
              the gcbench micro benchmark. We find that Rust's safety features do
              not create significant barriers to implementing a high performance
              collector. Though memory managers are usually considered low-level,
              our high performance implementation relies on very little unsafe
              code, with the vast majority of the implementation benefiting from
              Rust's safety. We see our experience as a compelling
              proof-of-concept of Rust as an implementation language for high
              performance garbage collection.},
  booktitle = {Proceedings of the 2016 ACM SIGPLAN International Symposium on
               Memory Management},
  pages = {89–98},
  numpages = {10},
  keywords = {memory management, garbage collection, Rust},
  location = {Santa Barbara, CA, USA},
  series = {ISMM 2016},
}


%5
@article{cho1998efficient,
  title = {Efficient crawling through URL ordering},
  author = {Cho, Junghoo and Garcia-Molina, Hector and Page, Lawrence},
  year = {1998},
}

@article{beautifulsoup4docs,
  title = {Beautiful Soup Documentation Release 4.4.0},
  author = {Leonard Richardson},
  year = {2019},
}

%6
@techreport{page1999pagerank,
  title = {The PageRank citation ranking: Bringing order to the web.},
  author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd,
            Terry},
  year = {1999},
  institution = {Stanford InfoLab},
}

%7
@inproceedings{brin1998extracting,
  title = {Extracting patterns and relations from the world wide web},
  author = {Brin, Sergey},
  booktitle = {International Workshop on The World Wide Web and Databases},
  pages = {172--183},
  year = {1998},
  organization = {Springer},
}

%8
@misc{william2001using,
  title = {Using Information Technology: A Practical introduction to computers
           and communications},
  author = {William, Brian K and Sawyer, Spacey C},
  year = {2001},
  publisher = {Boston: Mc Graw Hill},
}

%9
@inbook{najork2009web,
  author = {Najork, Marc},
  title = {Web Crawler Architecture},
  booktitle = {Encyclopedia of Database Systems},
  year = {2009},
  month = {September},
  abstract = {A web crawler is a program that, given one or more seed URLs,
              downloads the web pages associated with these URLs, extracts any
              hyperlinks contained in them, and recursively continues to download
              the web pages identified by these hyperlinks. Web crawlers are an
              important component of web search engines, where they are used to
              collect the corpus of web pages indexed by the search engine.
              Moreover, they are used in many other applications that process
              large numbers of web pages, such as web data mining, comparison
              shopping engines, and so on. Despite their conceptual simplicity,
              implementing high-performance web crawlers poses major engineering
              challenges due to the scale of the web. In order to crawl a
              substantial fraction of the “surface web” in a reasonable amount of
              time, web crawlers must download thousands of pages per second, and
              are typically distributed over tens or hundreds of computers.},
  publisher = {Springer Verlag},
  url = {
         https://www.microsoft.com/en-us/research/publication/web-crawler-architecture/
         },
  edition = {Encyclopedia of Database Systems},
}

%10
@article{salton1989automatic,
  title = {Automatic text processing: The transformation, analysis, and
           retrieval of},
  author = {Salton, Gerard},
  journal = {Reading: Addison-Wesley},
  volume = {169},
  year = {1989},
}

%11
@inproceedings{pinkerton1994finding,
  title = {Finding what people want: Experiences with the WebCrawler},
  author = {Pinkerton, Brian},
  booktitle = {Proc. 2nd WWW Conf., 1994},
  year = {1994},
}

%12
@electronic{netmarketshare,
  author = {NetMarketShare},
  title = {Search Engine Market Share},
  url = {http://engineering.purdue.edu/~mark/puthesis},
  lastchecked = {2020-9-15},
  year = {2020},
}

%13
@article{jonhenshaw2020,
  title = {Apple showing signs it may soon launch a search engine to compete
           against Google Search},
  url = {https://www.coywolf.news/seo/apple-search-engine/},
  journal = {Coywolf News},
  author = {Jon Henshaw},
  year = {2020},
  month = {Aug},
}

%14
@article{kaur2020simhar,
  title = {SIMHAR-Smart Distributed Web Crawler for the Hidden Web Using SIM+
           Hash and Redis Server},
  author = {Kaur, Sawroop and Geetha, G},
  journal = {IEEE Access},
  volume = {8},
  pages = {117582--117592},
  year = {2020},
  publisher = {IEEE},
}

%15
@inproceedings{castillo2005effective,
  title = {Effective web crawling},
  author = {Castillo, Carlos},
  booktitle = {Acm sigir forum},
  volume = {39},
  number = {1},
  pages = {55--56},
  year = {2005},
  organization = {Acm New York, NY, USA},
}

%16
@inproceedings{eichmann1994rbse,
  title = {The RBSE spider-balancing effective search against web load},
  author = {Eichmann, David},
  booktitle = {Proc. 1st WWW Conf},
  year = {1994},
  organization = {Citeseer},
}

%17
@electronic{technacenter,
  author = {TechnaCenterLLC},
  title = {Basic structure of an HTML document},
  url = {http://www.scriptingmaster.com/html/basic-structure-HTML-document.asp},
  lastchecked = {2020-10-01},
  year = {2020},
}

%18
@electronic{moz,
  author = {Moz},
  title = {URLs},
  url = {https://moz.com/learn/seo/url\#:~
         :text=A%20URL%20(Uniform%20Resource%20Locator,
         %2C%20HTTPS%2C%20FTP%2C%20etc.},
  lastchecked = {2020-10-05},
  year = {2020},
}

%19
@electronic{whatwg,
  author = {WHATWG},
  title = {URL Standard},
  url = {https://url.spec.whatwg.org/},
  lastchecked = {2020-10-05},
  year = {2020},
}

%20
@book{cormen2009introduction,
  title = {Introduction to algorithms},
  author = {Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and
            Stein, Clifford},
  year = {2009},
  publisher = {MIT press},
}

%21
@electronic{statista,
  author = {Statista},
  title = {Worldwide Market Share of Search Engine},
  url = {
         https://www.statista.com/statistics/216573/worldwide-market-share-of-search-engines/
         },
  lastchecked = {2020-10-13},
  year = {2020},
}

 %22
@book{operatingsystemconcept,
  title = {Operating System Concepts},
  author = {Abraham Silberchschatz, Peter Baer Galvin, Greg Gagne},
  year = {2018},
  publisher = {Willey},
}

@book{rustbook,
  title = {The Rust Programming Language},
  author = {Steve Klabnik, Carol Nichols},
  year = {2018},
  isbn = {1-59327-828-4},
  publisher = {William Pollock},
}

 %23
@article{lazuardithesis,
  title = {PERANCANGAN ARSITEKTUR SEARCH ENGINE DENGAN MENGINTEGRASIKAN WEB
           CRAWLER, ALGORITMA PAGE RANKING, DAN DOCUMENT RANKING},
  author = {Lazuardy Khatulistiwa},
  year = {2023},
}

@article{fathanthesis,
  title = {PERANCANGAN CRAWLER SEBAGAI PENDUKUNG PADA SEARCH ENGINE},
  author = {Muhammad Farthan Qorriba},
  year = {2021},
}

@electronic{bs4docs,
  author = {Statista},
  title = {Beautiful Soup Documentation},
  url = {https://www.crummy.com/software/BeautifulSoup/bs4/doc/},
  lastchecked = {2023-06-25},
  year = {2023},
}
